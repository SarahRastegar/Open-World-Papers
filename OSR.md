<!----------------------------------------------------------------------------------------------------------------------------------------------->
##  Open-Set Recognition Videos
<!----------------------------------------------------------------------------------------------------------------------------------------------->

### 2023 Papers
#### CVPR

* Enlarging Instance-specific and Class-specific Information for Open-set Action Recognition (CVPR 2023) 
[[Paper](https://arxiv.org/abs/2303.15467)]

* AutoLabel: CLIP-based framework for Open-set Video Domain Adaptation (CVPR 2023) 
[[Paper](https://arxiv.org/abs/2304.01110)]

* Open Set Action Recognition via Multi-Label Evidential Learning (CVPR 2023) 
[[Paper](https://arxiv.org/abs/2303.12698)]

* OpenGait: Revisiting Gait Recognition Towards Better Practicality (CVPR 2023) 
[[Paper](https://arxiv.org/abs/2211.06597)]

* Open-Category Human-Object Interaction Pre-training via Language Modeling Framework (CVPR 2023) 
[[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Zheng_Open-Category_Human-Object_Interaction_Pre-Training_via_Language_Modeling_Framework_CVPR_2023_paper.html)]

* SUDS: Scalable Urban Dynamic Scenes (CVPR 2023) 
[[Paper](https://arxiv.org/abs/2303.14536)]

<!-- #### ICLR
#### NeurIPS
#### ICCV
#### ICML
#### IEEE-Access
#### ECCV
#### AAAI
#### TPAMI
#### CVPRw-->
#### WACV
* Reconstructing Humpty Dumpty: Multi-Feature Graph Autoencoder for Open Set Action Recognition (WACV 2023) 
[[Paper](http://arxiv.org/abs/2212.06023)]
[[Code](https://github.com/Kitware/graphautoencoder)]<br>
*Datasets:  HMDB-51, UCF-101*<br>
*Task: Action Recognition*
<!--#### IJCV
#### BMVC
#### ICCw-->
#### Arxiv & Others

* Video Instance Segmentation in an Open-World (Arxiv 2023) 
[[Paper](https://arxiv.org/abs/2304.01200)]

* Plan4MC: Skill Reinforcement Learning and Planning for Open-World Minecraft Tasks (Arxiv 2023) 
[[Paper](https://arxiv.org/abs/2303.16563)]

* POAR: Towards Open-World Pedestrian Attribute Recognition (Arxiv 2023) 
[[Paper](https://arxiv.org/abs/2303.14643)]

* Learning to Operate in Open Worlds by Adapting Planning Models (AAMAS 2023) 
[[Paper](https://arxiv.org/abs/2303.14272)]

* PyReason: Software for Open World Temporal Logic (AAAI 2023) 
[[Paper](https://arxiv.org/abs/2302.13482)]

* NovPhy: A Testbed for Physical Reasoning in Open-world Environments (Arxiv 2023) 
[[Paper](https://arxiv.org/abs/2303.01711)]

* Improving Audio-Visual Video Parsing with Pseudo Visual Labels (Arxiv 2023) 
[[Paper](https://arxiv.org/abs/2303.02344)]

* Open-World Object Manipulation using Pre-trained Vision-Language Models (Arxiv 2023) 
[[Paper](https://arxiv.org/abs/2303.00905)]

* Towards Generalized Robot Assembly through Compliance-Enabled Contact Formations (ICRA 2023) 
[[Paper](https://arxiv.org/abs/2303.05565)]

* Discovering Novel Actions in an Open World with Object-Grounded Visual Commonsense Reasoning (Arxiv 2023) 
[[Paper](https://arxiv.org/abs/2305.16602)]

* Temporal-controlled Frame Swap for Generating High-Fidelity Stereo Driving Data for Autonomy Analysis (Arxiv 2023) 
[[Paper](https://arxiv.org/abs/2306.01704)]


<!----------------------------------------------------------------------------------------------------------------------------------------------->
### 2022 Papers
#### CVPR
* Opening Up Open World Tracking (CVPR 2022) 
[[Paper](https://openaccess.thecvf.com/content/CVPR2022/papers/Liu_Opening_Up_Open_World_Tracking_CVPR_2022_paper.pdf)]
[[Code](https://openworldtracking.github.io)]<br>
*Datasets: TAO-OW*<br>
*Task: Object Tracking*

* OpenTAL: Towards Open Set Temporal Action Localization (CVPR 2022) 
[[Paper](https://openaccess.thecvf.com/content/CVPR2022/papers/Bao_OpenTAL_Towards_Open_Set_Temporal_Action_Localization_CVPR_2022_paper.pdf)]
[[Code](https://www.rit.edu/actionlab/opental)]<br>
*Datasets: THUMOS14, ActivityNet1.3*<br>
*Task: Temporal Action Localization*

* UBnormal: New Benchmark for Supervised Open-Set Video Anomaly Detection (CVPR 2022) 
[[Paper](https://openaccess.thecvf.com/content/CVPR2022/papers/Acsintoae_UBnormal_New_Benchmark_for_Supervised_Open-Set_Video_Anomaly_Detection_CVPR_2022_paper.pdf)]
[[Code](https://github.com/lilygeorgescu/UBnormal)]<br>
*Datasets: UBnormal, CHUK, Avenue, Shang-hai Tech*<br>
*Task: Anomaly Detection*

* Open-World Instance Segmentation: Exploiting Pseudo Ground Truth From Learned Pairwise Affinity (CVPR 2022) 
[[Paper](https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_Open-World_Instance_Segmentation_Exploiting_Pseudo_Ground_Truth_From_Learned_Pairwise_CVPR_2022_paper.pdf)]<br>
*Datasets: COCO 17, LVIS, UVO (videos), ADE20k*<br>
*Task: Instance Segmentation*


<!--#### ICLR
#### NeurIPS
#### ICCV
#### ICML
#### IEEE-Access-->
#### ECCV
* Towards Open Set Video Anomaly Detection (ECCV 2022) 
[[Paper](https://arxiv.org/pdf/2208.11113v1.pdf)]<br>
*Datasets: XD Violence, UCF Crime, ShanghaiTech Campus*<br>
*Task: Anomaly Detection*

<!--#### AAAI
#### TPAMI
#### CVPRw
#### WACV
#### IJCV
#### BMVC
#### ICCw-->
#### Arxiv & Others
* Human Activity Recognition in an Open World (Submitted to JAIR 2022) 
[[Paper](https://arxiv.org/abs/2212.12141)]

* Self-Initiated Open World Learning for Autonomous AI Agents (AAAI 2022 Spring Symposium Series) 
[[Paper](https://arxiv.org/abs/2110.11385)] 

* UVO Challenge on Video-based Open-World Segmentation 2021: 1st Place Solution (Arxiv 2022) 
[[Paper](https://arxiv.org/abs/2110.11661)] 

<!----------------------------------------------------------------------------------------------------------------------------------------------->
### 2021 Papers
#### CVPR
* Generalizing to the Open World: Deep Visual Odometry With Online Adaptation (CVPR 2021) 
[[Paper](https://openaccess.thecvf.com/content/CVPR2021/papers/Li_Generalizing_to_the_Open_World_Deep_Visual_Odometry_With_Online_CVPR_2021_paper.pdf)]<br>
*Datasets: Cityscapes,  KITTI, indoor TUM, NYUv2*<br>
*Task: Depth Estimation*


<!--#### ICLR
#### NeurIPS-->
#### ICCV
* Evidential Deep Learning for Open Set Action Recognition (ICCV 2021) 
[[Paper](https://arxiv.org/pdf/2107.10161v2.pdf)]
[[Code](https://www.rit.edu/actionlab/dear)]<br>
*Datasets: UCF-101, HMDB-51, MiT-v2*<br>
*Task: Action Recognition*

* Unidentified Video Objects: A Benchmark for Dense, Open-World Segmentation (ICCV 2021) 
[[Paper](https://openaccess.thecvf.com/content/ICCV2021/papers/Wang_Unidentified_Video_Objects_A_Benchmark_for_Dense_Open-World_Segmentation_ICCV_2021_paper.pdf)]<br>
*Datasets: UVO, COCO*<br>
*Task: Video Object Detection and Segmentation*

<!-- #### ICML
#### IEEE-Access
#### ECCV
#### AAAI
#### TPAMI
#### CVPRw
#### WACV
#### IJCV
#### BMVC
#### ICCw
#### Arxiv & Others-->

* Conditional Extreme Value Theory for Open Set Video Domain Adaptation (MMAsia 2021) 
[[Paper](https://dl.acm.org/doi/10.1145/3469877.3490600)]<br>

* Dual Metric Discriminator for Open Set Video Domain Adaptation (ICASSP 2021) 
[[Paper](https://ieeexplore.ieee.org/document/9413361)]<br> 

* Open-World Active Learning with Stacking Ensemble for Self-Driving Cars (Arxiv 2021) 
[[Paper](https://arxiv.org/abs/2109.06628)] 

* Physical Reasoning in an Open World (ACS 2021) 
[[Paper](https://arxiv.org/abs/2201.08950)] 

* Person Re-identification based on Robust Features in Open-world (Arxiv 2021) 
[[Paper](https://arxiv.org/abs/2102.10798)] 

* Online Action Recognition (AAAI 2021) 
[[Paper](https://arxiv.org/abs/2012.07464)] 

* ViNG: Learning Open-World Navigation with Visual Goals (ICRA 2021) 
[[Paper](https://arxiv.org/abs/2012.09812)] 

<!----------------------------------------------------------------------------------------------------------------------------------------------->
### Older Papers
* Specifying weight priors in bayesian deep neural networks with empirical bayes (AAAI 2020) 
[[Paper](https://arxiv.org/pdf/1906.05323v3.pdf)]<br>
*Datasets: UCF-101, Urban Sound 8K, MNIST, Fashion-MNIST, CIFAR10*<br>
*Task: Image and Audio Classification, Video Activity Recognition*

* P-ODN: prototype-based open Deep network for open Set Recognition (Scientific Reports 2020) 
[[Paper](https://www.nature.com/articles/s41598-020-63649-6)]<br>
*Datasets: UCF11, UCF50, UCF101 and HMDB51*<br>
*Task: Action Recognition*

* Uncertainty-aware audiovisual activity recognition using deep bayesian variational inference (ICCV 2019) 
[[Paper](https://arxiv.org/pdf/1811.10811v3.pdf)]<br>
*Datasets: MiT*<br>
*Task: Audiovisual Action Recognition*

* Bayesian activity recognition using variational inference (NeurIPS 2018) 
[[Paper](https://arxiv.org/pdf/1811.03305v2.pdf)]<br>
*Datasets:  MiT video activity recognition dataset*<br>
*Task: Action Recognition*

* ODN: Opening the deep network for open-set action recognition (ICME 2018) 
[[Paper](https://arxiv.org/pdf/1901.07757v1.pdf)]<br>
*Datasets:  HMDB51, UCF50, UCF101*<br>
*Task: Action Recognition*

* Open-World Stereo Video Matching with Deep RNN (ECCV 2018) 
[[Paper](https://www.ecva.net/papers/eccv_2018/papers_ECCV/papers/Yiran_Zhong_Open-World_Stereo_Video_ECCV_2018_paper.pdf)]<br>
*Datasets: KITTI VO, Middlebury Stereo 2005 & 2006, Freiburg Sceneflow, Random dot, Synthia*<br>
*Task: Stereo Video Matching*

* Adversarial Open-World Person Re-Identification (ECCV 2018) 
[[Paper](https://www.ecva.net/papers/eccv_2018/papers_ECCV/papers/Xiang_Li_Adversarial_Open-World_Person_ECCV_2018_paper.pdf)]<br>
*Datasets: Market-1501, CUHK01, CUHK03*<br>
*Task: Person Re-Identification*

* From Open Set to Closed Set: Counting Objects by Spatial Divide-and-Conquer (ICCV 2019) 
[[Paper](https://openaccess.thecvf.com/content_ICCV_2019/papers/Xiong_From_Open_Set_to_Closed_Set_Counting_Objects_by_Spatial_ICCV_2019_paper.pdf)]
[[Code](https://github.com/xhp-hust-2018-2011/S-DCNet)]<br>
*Datasets: Synthesized Cell Counting, UCF-QNRF, ShanghaiTech, UCFCC50, TRANCOS and MTC*<br>
*Task: Visual Counting*

* AutOTranS: an Autonomous Open World Transportation System (Arxiv 2018) 
[[Paper](https://arxiv.org/abs/1810.03400)] 

* From Known to the Unknown: Transferring Knowledge to Answer Questions about Novel Visual and Semantic Concepts (Arxiv 2018) 
[[Paper](https://arxiv.org/abs/1811.12772)] 

* Visual Curiosity: Learning to Ask Questions to Learn Visual Recognition (CoRL 2018 Oral) 
[[Paper](https://arxiv.org/abs/1810.00912)] 

* Towards Large-Scale Video Video Object Mining (ECCVw 2018) 
[[Paper](https://arxiv.org/abs/1809.07316)] 
